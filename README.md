# transformers_from_scratch

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelï¼Œç®€ç§°LLMï¼‰ï¼ŒæŒ‡ä½¿ç”¨å¤§é‡æ–‡æœ¬æ•°æ®è®­ç»ƒçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¯ä»¥ç”Ÿæˆè‡ªç„¶è¯­è¨€æ–‡æœ¬æˆ–ç†è§£è¯­è¨€æ–‡æœ¬çš„å«ä¹‰ã€‚

![llm](./images/llm.png)

è™½ç„¶ç½‘ä¸Šæœ‰å¤§é‡å…³äºtransformerç†è®ºã€å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒçš„æ•™ç¨‹ã€‚ä½†æ˜¯å°‘æœ‰å…³äºé¢„è®­ç»ƒçš„è§£é‡Šã€‚æœ¬æ–‡åˆ™ä»å¦‚ä½•è‡ªå·±å®æˆ˜é¢„è®­ç»ƒä¸€ä¸ªå¤§è¯­è¨€æ¨¡å‹çš„è§’åº¦ï¼Œä½¿ç”¨wikiæ•°æ®é›†è¿›è¡Œä¸€ä¸ªç®€å•çš„ä»é›¶é¢„è®­ç»ƒå·¥ä½œï¼Œå¹¶é™„ä¸Šä½¿ç”¨swanlab launchç™½å«–æ˜¾å¡çš„æ–¹æ³•

* å®éªŒè®°å½•ï¼š[SwanLab](https://swanlab.cn/@ShaohonChen/WikiLLM/overview)

* æ•°æ®é›†ä¸‹è½½ï¼š[ç™¾åº¦ç½‘ç›˜](.link)ï¼Œ[huggingface](https://huggingface.co/datasets/fjcanyue/wikipedia-zh-cn)

---

## å®‰è£…ç¯å¢ƒ

é¦–å…ˆï¼Œé¡¹ç›®æ¨èä½¿ç”¨python3.10ã€‚éœ€è¦å®‰è£…çš„pythonåŒ…å¦‚ä¸‹ï¼š

```txt
swanlab
transformers
datasets
accelerate
```

ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤ä¸€é”®å®‰è£…ï¼š

```bash
pip install swanlab transformers datasets accelerate modelscope
```

---

## ä¸‹è½½æ•°æ®é›†

æœ¬æ•™ç¨‹ä½¿ç”¨çš„æ˜¯ä¸­æ–‡wikiæ•°æ®ï¼Œç†è®ºä¸Šé¢„è®­ç»ƒæ•°æ®é›†ç§ç±»è¶Šä¸°å¯Œã€æ•°æ®é‡è¶Šå¤§è¶Šå¥½ï¼Œåç»­ä¼šå¢åŠ åˆ«çš„æ•°æ®é›†ã€‚

![dataset](./images/dataset.png)

huggingfaceé“¾æ¥ï¼š[wikipedia-zh-cn](https://huggingface.co/datasets/fjcanyue/wikipedia-zh-cn)

ç™¾åº¦ç½‘ç›˜ä¸‹è½½åœ°å€ï¼š[ç™¾åº¦ç½‘ç›˜](.link)

ä¸‹è½½`wikipedia-zh-cn-20240820.json`æ–‡ä»¶åæ”¾åˆ°é¡¹ç›®ç›®å½•ä¸‹`./data/`æ–‡ä»¶å¤¹ä¸­

è¯¥æ•°æ®é›†æ–‡ä»¶çº¦1.99Gå¤§ï¼Œå…±æœ‰1.44Mæ¡æ•°æ®ã€‚è™½ç„¶æ•°æ®é›†ä¸­åŒ…å«æ–‡ç« æ ‡é¢˜ï¼Œä½†æ˜¯å®é™…ä¸Šåœ¨é¢„è®­ç»ƒé˜¶æ®µç”¨ä¸ä¸Šã€‚æ­£æ–‡ç‰‡æ®µå‚è€ƒï¼š

```txt
æ•°å­¦æ˜¯ç ”ç©¶æ•°é‡ã€ç»“æ„ä»¥åŠç©ºé—´ç­‰æ¦‚å¿µåŠå…¶å˜åŒ–çš„ä¸€é—¨å­¦ç§‘ï¼Œå±äºå½¢å¼ç§‘å­¦çš„ä¸€ç§ã€‚æ•°å­¦åˆ©ç”¨æŠ½è±¡åŒ–å’Œé€»è¾‘æ¨ç†ï¼Œä»è®¡æ•°ã€è®¡ç®—ã€é‡åº¦ã€å¯¹ç‰©ä½“å½¢çŠ¶åŠè¿åŠ¨çš„è§‚å¯Ÿå‘å±•è€Œæˆã€‚æ•°å­¦å®¶ä»¬æ‹“å±•è¿™äº›æ¦‚å¿µ...
```

ä½¿ç”¨[ğŸ¤—Huggingface Datasets](https://huggingface.co/docs/datasets/index)åŠ è½½æ•°æ®é›†çš„ä»£ç å¦‚ä¸‹ï¼š

```python
from datasets import load_dataset

ds = load_dataset("fjcanyue/wikipedia-zh-cn")
```

å¦‚æœä½¿ç”¨ç™¾åº¦ç½‘ç›˜ä¸‹è½½çš„jsonæ–‡ä»¶ï¼Œå¯ä»¥é€šè¿‡å¦‚ä¸‹ä»£ç åŠ è½½

```python
raw_datasets = datasets.load_dataset(
    "json", data_files="data/wikipedia-zh-cn-20240820.json"
)

raw_datasets = raw_datasets["train"].train_test_split(test_size=0.1, seed=2333)
print("dataset info")
print(raw_datasets)
```

---

## è¿è¡Œè®­ç»ƒ

è¿è¡Œå¦‚ä¸‹å‘½ä»¤

```
python pretrain.py
```

å¯ä»¥çœ‹åˆ°å¦‚ä¸‹è®­ç»ƒæ—¥å¿—ã€‚ç”±äºè®­ç»ƒæ—¶é—´è¾ƒé•¿ï¼Œæ¨èä½¿ç”¨tmuxå°†è®­ç»ƒä»»åŠ¡holdä½

![terminal](./images/terminal.png)

å¯ä»¥åœ¨[SwanLab](https://swanlab.cn)ä¸­æŸ¥çœ‹æœ€ç»ˆçš„è®­ç»ƒç»“æœï¼š

![log](./images/log.png)
